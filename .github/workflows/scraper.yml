name: Costa Rica Jobs Scraper

on:
  # Run daily at 9:00 AM UTC
  schedule:
    - cron: '0 9 * * *'
  
  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml
      
      - name: Run scraper
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            MODE="${{ inputs.scrape_mode }}"
          else
            MODE="weekly"
          fi
          
          echo "Running scraper in $MODE mode"
          
          if [ "$MODE" = "initial_full" ]; then
            python -c "from first_page_scraper import initial_scrape; initial_scrape()"
          elif [ "$MODE" = "two_pages" ]; then
            python -c "from first_page_scraper import scrape_two_pages_only; scrape_two_pages_only()"
          else
            python -c "from first_page_scraper import weekly_update; weekly_update()"
          fi
      
      - name: Configure Git
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
      
      - name: Commit and push if changes
        run: |
          git add *.json *.csv
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ðŸ¤– Auto-update: Costa Rica jobs data ($(date +'%Y-%m-%d %H:%M:%S'))"
            git push
          fi
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scraped-jobs-${{ github.run_number }}
          path: |
            *.json
            *.csv
          retention-days: 30
      
      - name: Create summary
        if: always()
        run: |
          echo "## ðŸ“Š Scraping Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Date**: $(date +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "- **Mode**: ${{ inputs.scrape_mode || 'weekly (scheduled)' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f costa_rica_jobs_full.json ]; then
            COUNT=$(python -c "import json; print(len(json.load(open('costa_rica_jobs_full.json'))))")
            echo "- **Total jobs in database**: $COUNT" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“ Generated Files" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          ls -lh *.json *.csv 2>/dev/null | awk '{print "- `" $9 "` - " $5}' >> $GITHUB_STEP_SUMMARY || echo "No files generated" >> $GITHUB_STEP_SUMMARY

  notify-failure:
    runs-on: ubuntu-latest
    needs: scrape-jobs
    if: failure()
    
    steps:
      - name: Create failure notification
        run: |
          echo "## âš ï¸ Scraping Failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The job scraping workflow encountered an error." >> $GITHUB_STEP_SUMMARY
          echo "Please check the logs for details." >> $GITHUB_STEP_SUMMARY
